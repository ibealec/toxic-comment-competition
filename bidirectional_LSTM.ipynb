{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model With Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas as pd\n",
    "import helpers as hlp\n",
    "\n",
    "\n",
    "def make_df(train_path, test_path, max_features, maxlen, list_classes):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    train = train.sample(frac=1)\n",
    "\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\n",
    "    y = train[list_classes].values\n",
    "    list_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values\n",
    "    \n",
    "    #list_sentences_train = hlp.clean(list_sentences_train)\n",
    "    #list_sentences_test = hlp.clean(list_sentences_test)\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    return X_t, X_te, y, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "def BidLstm(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(512, return_sequences=True, dropout=0.2,\n",
    "                           recurrent_dropout=0.2))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_glovevec(glovepath, max_features, embed_size, word_index, veclen=300):\n",
    "    embeddings_index = {}\n",
    "    f = open(glovepath)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ' '.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs.reshape(-1)\n",
    "    f.close()\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(embedding_matrix)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.27204001 -0.06203    -0.1884     ...,  0.13015001 -0.18317001  0.1323    ]\n",
      " [ 0.31924     0.06316    -0.27858001 ...,  0.082745    0.097801\n",
      "   0.25044999]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-0.37496999 -0.37419999  0.067547   ..., -0.026452   -0.23654    -0.037388  ]]\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9757\n",
      "Epoch 00001: val_loss improved from inf to 0.04923, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0755 - acc: 0.9758 - val_loss: 0.0492 - val_acc: 0.9821\n",
      "Epoch 2/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9819\n",
      "Epoch 00002: val_loss improved from 0.04923 to 0.04534, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 163s 1ms/step - loss: 0.0487 - acc: 0.9819 - val_loss: 0.0453 - val_acc: 0.9828\n",
      "Epoch 3/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9827\n",
      "Epoch 00003: val_loss improved from 0.04534 to 0.04352, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 163s 1ms/step - loss: 0.0457 - acc: 0.9827 - val_loss: 0.0435 - val_acc: 0.9836\n",
      "Epoch 4/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9833\n",
      "Epoch 00004: val_loss improved from 0.04352 to 0.04317, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 163s 1ms/step - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0432 - val_acc: 0.9836\n",
      "Epoch 5/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9840\n",
      "Epoch 00005: val_loss improved from 0.04317 to 0.04189, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0414 - acc: 0.9840 - val_loss: 0.0419 - val_acc: 0.9843\n",
      "Epoch 6/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9844\n",
      "Epoch 00006: val_loss improved from 0.04189 to 0.04124, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0399 - acc: 0.9844 - val_loss: 0.0412 - val_acc: 0.9842\n",
      "Epoch 7/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9846\n",
      "Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0452 - val_acc: 0.9829\n",
      "Epoch 8/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9847\n",
      "Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0414 - val_acc: 0.9844\n",
      "Epoch 9/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9849\n",
      "Epoch 00009: val_loss improved from 0.04124 to 0.04097, saving model to .model.hdf5\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0390 - acc: 0.9849 - val_loss: 0.0410 - val_acc: 0.9844\n",
      "Epoch 10/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9857\n",
      "Epoch 00010: val_loss did not improve\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0362 - acc: 0.9857 - val_loss: 0.0417 - val_acc: 0.9845\n",
      "Epoch 11/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9859\n",
      "Epoch 00011: val_loss did not improve\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0429 - val_acc: 0.9844\n",
      "Epoch 12/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9865\n",
      "Epoch 00012: val_loss did not improve\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0335 - acc: 0.9865 - val_loss: 0.0423 - val_acc: 0.9845\n",
      "Predicting with model...\n",
      "Saving to submission file...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_features = 100000\n",
    "    maxlen = 150\n",
    "    embed_size = 300\n",
    "    list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\",\n",
    "                    \"identity_hate\"]\n",
    "\n",
    "    xtr, xte, y, word_index = make_df(\"./input/train.csv\",\n",
    "                                      \"./input/test.csv\",\n",
    "                                      max_features, maxlen, list_classes)\n",
    "    embedding_vector = make_glovevec(\"./input/glove.840B.300d.txt\",\n",
    "                                     max_features, embed_size, word_index)\n",
    "\n",
    "    model = BidLstm(maxlen, max_features, embed_size, embedding_vector)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    file_path = \".model.hdf5\"\n",
    "    ckpt = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                           save_best_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "    model.fit(xtr, y, batch_size=512, epochs=20, validation_split=0.1, callbacks=[ckpt, early])\n",
    "    #model.fit(xtr, y, batch_size=256, epochs=1, validation_split=0.1)\n",
    "    model.load_weights(file_path)\n",
    "    print(\"Predicting with model...\")\n",
    "    y_test = model.predict(xte)\n",
    "    sample_submission = pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    sample_submission[list_classes] = y_test\n",
    "    print(\"Saving to submission file...\")\n",
    "    sample_submission.to_csv(\"lstm_sub.csv\", index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fc9cbbfb7565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./input/sample_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sub.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights(file_path)\n",
    "y_test = model.predict(xte)\n",
    "sample_submission = pd.read_csv(\"./input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"submissions/bidlstm_with_attention.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
